{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17c741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39179294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.f_utils.embedding_utils import load_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "662b339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>56699142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  study_id  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0    10000032  50414267          NaN           NaN            NaN    NaN   \n",
       "1    10000032  53189527          NaN           NaN            NaN    NaN   \n",
       "2    10000032  53911762          NaN           NaN            NaN    NaN   \n",
       "3    10000032  56699142          NaN           NaN            NaN    NaN   \n",
       "4    10000764  57375967          NaN           NaN            1.0    NaN   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         NaN       NaN          NaN           NaN   \n",
       "1                         NaN       NaN          NaN           NaN   \n",
       "2                         NaN       NaN          NaN           NaN   \n",
       "3                         NaN       NaN          NaN           NaN   \n",
       "4                         NaN       NaN          NaN           NaN   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         1.0               NaN            NaN        NaN           NaN   \n",
       "1         1.0               NaN            NaN        NaN           NaN   \n",
       "2         1.0               NaN            NaN        NaN           NaN   \n",
       "3         1.0               NaN            NaN        NaN           NaN   \n",
       "4         NaN               NaN            NaN       -1.0           NaN   \n",
       "\n",
       "   Support Devices  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = pd.read_csv(\"../dados/mimic/mimic-cxr-2.0.0-chexpert.csv\")\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38b9ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835,)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index('artifacts/vector_store/faiss_img_aux.index')\n",
    "\n",
    "# ids\n",
    "ids = load_embeddings('artifacts/embeddings/study_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81db8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.f_utils.evaluation import *\n",
    "from src.f_utils.rag_search import search_relevant_cases\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c93c16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.fillna(0, inplace=True)\n",
    "binary_cols = [col for col in df_labels.columns if col not in ['subject_id', 'study_id']]\n",
    "for c in binary_cols:\n",
    "    df_labels[c] = np.where(df_labels[c] == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "978b9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.f_utils.mimic_labels import _get_gabarito_any, _get_gabarito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d99f4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.f_utils.embedding_utils import load_embeddings, extract_embedding_single_study, extract_embeddings_from_img, extract_embeddings_from_text, _extract_findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9230b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (377110,)\n"
     ]
    }
   ],
   "source": [
    "emb_per_image = load_embeddings(\"artifacts/img_embeddings/embeddings_per_image.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d51d7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (377110, 3)\n"
     ]
    }
   ],
   "source": [
    "metadata = load_embeddings(\"artifacts/img_embeddings/image_metadata.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "838231ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_study_dataset(\n",
    "        emb_per_image, \n",
    "        study_ids_path=\"artifacts/embeddings/study_ids.npy\", \n",
    "        patient_ids_path=\"artifacts/embeddings/patient_ids.npy\",\n",
    "        text_emb_path=\"artifacts/embeddings/e_text.npy\",\n",
    "        labels_path=\"../dados/mimic/mimic-cxr-2.0.0-chexpert.csv\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Agrupa embeddings de imagens por estudo e combina com embeddings de texto e labels.\n",
    "    \n",
    "    Args:\n",
    "        emb_per_image: lista de dicion√°rios com embeddings por imagem\n",
    "        study_ids_path: caminho para arquivo com IDs dos estudos\n",
    "        patient_ids_path: caminho para arquivo com IDs dos pacientes\n",
    "        text_emb_path: caminho para arquivo com embeddings de texto\n",
    "        labels_path: caminho para arquivo CSV com labels CheXpert\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dicion√°rios, um por estudo\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Carregar dados alinhados por √≠ndice\n",
    "    study_ids = load_embeddings(study_ids_path)\n",
    "    patient_ids = load_embeddings(patient_ids_path)\n",
    "    text_embeddings = load_embeddings(text_emb_path)\n",
    "    \n",
    "    # Carregar labels\n",
    "    df_labels = pd.read_csv(labels_path)\n",
    "    \n",
    "    # Identificar colunas de labels (excluindo subject_id e study_id)\n",
    "    label_columns = [col for col in df_labels.columns if col not in ['subject_id', 'study_id']]\n",
    "    \n",
    "    # Criar mapeamento study_id -> labels\n",
    "    study_labels_dict = {}\n",
    "    for _, row in df_labels.iterrows():\n",
    "        study_id = 's' + str(int(row['study_id']))\n",
    "        # Pegar labels onde o valor √© 1\n",
    "        labels = [col for col in label_columns if row[col] == 1.0]\n",
    "        study_labels_dict[study_id] = labels\n",
    "    \n",
    "    # Criar mapeamento study_id -> index\n",
    "    study_id_to_index = {sid: idx for idx, sid in enumerate(study_ids)}\n",
    "    \n",
    "    # Agrupar imagens por study_id\n",
    "    from collections import defaultdict\n",
    "    studies_dict = defaultdict(lambda: {\n",
    "        'files_images': [],\n",
    "        'embedding_images': []\n",
    "    })\n",
    "    \n",
    "    for img_data in emb_per_image:\n",
    "        study_id = img_data['study_id']\n",
    "        studies_dict[study_id]['files_images'].append(img_data['image_name'])\n",
    "        studies_dict[study_id]['embedding_images'].append(img_data['embedding_image'])\n",
    "        studies_dict[study_id]['patient_id'] = img_data['patient_id']\n",
    "    \n",
    "    # Construir lista final de estudos\n",
    "    result = []\n",
    "    for study_id, data in studies_dict.items():\n",
    "        # Buscar o √≠ndice do estudo\n",
    "        if study_id not in study_id_to_index:\n",
    "            print(f\"‚ö†Ô∏è Study ID {study_id} n√£o encontrado nos arquivos de embeddings\")\n",
    "            continue\n",
    "            \n",
    "        study_idx = study_id_to_index[study_id]\n",
    "        \n",
    "        study_dict = {\n",
    "            \"study_index\": study_idx,\n",
    "            \"study_id\": study_id,\n",
    "            \"patient_id\": patient_ids[study_idx],\n",
    "            \"files_images\": data['files_images'],\n",
    "            \"embedding_images\": data['embedding_images'],\n",
    "            \"file_text\": f\"{study_id}.txt\",\n",
    "            \"embedding_text\": text_embeddings[study_idx],\n",
    "            \"study_labels\": study_labels_dict.get(study_id, [])  # Labels do estudo ou lista vazia\n",
    "        }\n",
    "        result.append(study_dict)\n",
    "    \n",
    "    print(f\"‚úÖ {len(result)} estudos processados\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bff2b9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835,)\n",
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835,)\n",
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835, 1152)\n",
      "‚úÖ 227835 estudos processados\n",
      "\n",
      "\n",
      "Total de estudos: 227835\n",
      "Exemplo: {'study_index': 0, 'study_id': 's50414267', 'patient_id': 'p10000032', 'files_images': ['02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg', '174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg'], 'embedding_images': [array([-0.02441736,  0.03465954,  0.00766411, ..., -0.01386673,\n",
      "        0.06212181,  0.01484995], dtype=float32), array([-0.03161281,  0.03163843,  0.00890976, ..., -0.01544448,\n",
      "        0.05352064,  0.01552592], dtype=float32)], 'file_text': 's50414267.txt', 'embedding_text': array([-0.01413504,  0.01464215, -0.02659141, ..., -0.03464624,\n",
      "       -0.07638288, -0.01550421], dtype=float32), 'study_labels': ['No Finding']}\n"
     ]
    }
   ],
   "source": [
    "studies_dataset = build_study_dataset(emb_per_image)\n",
    "print(f\"\\n\\nTotal de estudos: {len(studies_dataset)}\")\n",
    "print(f\"Exemplo: {studies_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65c7ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AQUI PODEMOS TESTAR VARIOS ALPHAS\n",
    "def calculate_study_embedding(emb_text, emb_images, alpha=0.5):\n",
    "    # Convert to tensors if they are numpy arrays\n",
    "    if isinstance(emb_text, np.ndarray):\n",
    "        emb_text = torch.from_numpy(emb_text)\n",
    "    \n",
    "    # Convert list of numpy arrays to tensors\n",
    "    if isinstance(emb_images[0], np.ndarray):\n",
    "        emb_images = [torch.from_numpy(img) for img in emb_images]\n",
    "    \n",
    "    # === 3) faz pooling com imagens de entrada\n",
    "    # Stacking para [N, D]\n",
    "    emb_images = torch.stack(emb_images)  # [num_imagens, embedding_dim]\n",
    "    \n",
    "    # Pooling (m√©dia) ao longo das imagens\n",
    "    emb_pool = emb_images.mean(dim=0)  # [embedding_dim]\n",
    "    emb_pool = emb_pool / emb_pool.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # === 4) fez media dos embeddings para embedding final\n",
    "    e_study = alpha * emb_text + (1 - alpha) * emb_pool\n",
    "    e_study = e_study / e_study.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return e_study.detach().cpu().numpy()  # Return as numpy for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "579a7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 227835/227835 [2:12:04<00:00, 28.75it/s]  \n"
     ]
    }
   ],
   "source": [
    "# para cada item de val_dataset, calcular os top k\n",
    "# k=2\n",
    "\n",
    "k=5\n",
    "\n",
    "# queries = [] # lista de sets de labels para cada estudo de consulta\n",
    "# retrived = [] # lista de listas de sets (resultados por query) retrieved[i][j] = labels do resultado j da query i\n",
    "\n",
    "results = [] # Salva todos os resultados nessa lista\n",
    "y_true = []\n",
    "y_pred = []\n",
    "binary_cols = [col for col in df_labels.columns if col not in ['subject_id', 'study_id']]\n",
    "\n",
    "for study in tqdm(studies_dataset):\n",
    "    \n",
    "    # Get the study vector from the original index\n",
    "    study_vector = study['embedding_images'][0] # ajustei a fun√ß√£o para pular o primeiro caso\n",
    "\n",
    "    # emb_images = [torch.from_numpy(img) for img in study_vector]\n",
    "    # # Stacking para [N, D]\n",
    "    # emb_images = torch.stack(emb_images)  # [num_imagens, embedding_dim]\n",
    "\n",
    "    # # Pooling (m√©dia) ao longo das imagens\n",
    "    # emb_pool = emb_images.mean(dim=0)  # [embedding_dim]\n",
    "    # emb_pool = emb_pool / emb_pool.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Search the filtered index for the top k most similar vectors\n",
    "    estudos, idxs = search_relevant_cases(study_vector, index, ids, k)\n",
    "\n",
    "    # Cria o gabarito para mandar pra fun√ß√£o\n",
    "    try:\n",
    "        gabarito = _get_gabarito(int(study['study_id'].replace('s', '')), df_labels)\n",
    "    except:\n",
    "        gabarito = df_labels\n",
    "\n",
    "    # Criar mapeamento study_id -> labels\n",
    "    gabarito_list_set = np.repeat(0, gabarito.shape[0])\n",
    "\n",
    "    # Get the labels of the top k most similar vectors\n",
    "    study_top_k = []\n",
    "    for i in idxs:\n",
    "        study_top_k.append({\n",
    "            'study_id': studies_dataset[i]['study_index'],\n",
    "            'study_id': studies_dataset[i]['study_id'],\n",
    "            'labels': studies_dataset[i]['study_labels']\n",
    "        })\n",
    "\n",
    "    ground_truth_labels = set(study['study_labels'])\n",
    "    list_set_top_k = [(set(s['labels'])) for s in study_top_k]\n",
    "\n",
    "    metrics = evaluate_single_query(ground_truth_labels, list_set_top_k, gabarito_list_set, k)\n",
    "\n",
    "    results.append(metrics)\n",
    "    \n",
    "    # True labels repeated\n",
    "    for s in study_top_k:\n",
    "        try: \n",
    "            y_true_i = df_labels.loc[df_labels.study_id == int(study['study_id'].replace('s', ''))][binary_cols].values.tolist()[0]\n",
    "            y_pred_i = df_labels.loc[df_labels.study_id == int(s['study_id'].replace('s', ''))][binary_cols].values.tolist()[0]\n",
    "            y_true.append(y_true_i)\n",
    "            y_pred.append(y_pred_i)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# M√©dia final das m√©tricas\n",
    "mean_metrics = {\n",
    "    m: np.mean([res[m] for res in results])\n",
    "    for m in results[0]\n",
    "}\n",
    "\n",
    "# results,mean_metrics = evaluate_dataset(queries, retrived, k=k) # avaliar o dataset inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "739988ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@k': 0.4222213443939692,\n",
       " 'recall@k': 0.02084429653845491,\n",
       " 'jaccard_1@k': 0.14184826738648582,\n",
       " 'jaccard@k': 0.23704875518918578,\n",
       " 'ndcg@k': 0.6011642801744962}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98e7c3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               Atelectasis       0.25      0.25      0.25    229037\n",
      "              Cardiomegaly       0.25      0.24      0.24    224223\n",
      "             Consolidation       0.07      0.07      0.07     53885\n",
      "                     Edema       0.18      0.17      0.18    135086\n",
      "Enlarged Cardiomediastinum       0.05      0.05      0.05     35894\n",
      "                  Fracture       0.03      0.03      0.03     21949\n",
      "               Lung Lesion       0.04      0.04      0.04     31419\n",
      "              Lung Opacity       0.25      0.25      0.25    257619\n",
      "                No Finding       0.42      0.44      0.43    377260\n",
      "          Pleural Effusion       0.31      0.30      0.31    271494\n",
      "             Pleural Other       0.02      0.02      0.02     10055\n",
      "                 Pneumonia       0.08      0.08      0.08     82776\n",
      "              Pneumothorax       0.07      0.07      0.07     51789\n",
      "           Support Devices       0.45      0.45      0.45    332782\n",
      "\n",
      "                 micro avg       0.29      0.29      0.29   2115268\n",
      "                 macro avg       0.18      0.18      0.18   2115268\n",
      "              weighted avg       0.29      0.29      0.29   2115268\n",
      "               samples avg       0.30      0.30      0.28   2115268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names = binary_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f84cd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Salvar balanced_validation_dataset em artifacts/datasets\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs(\"artifacts/resultados_aux\", exist_ok=True)\n",
    "\n",
    "# Salvar o dataset de valida√ß√£o balanceado\n",
    "np.save(\"artifacts/resultados/results_img.npy\", results)\n",
    "np.save(\"artifacts/resultados/y_pred_img.npy\", y_pred)\n",
    "np.save(\"artifacts/resultados/y_true_img.npy\", y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82d08046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@k': 0.4238119691882282,\n",
       " 'recall@k': 0.020452094793698875,\n",
       " 'jaccard_1@k': 0.14752167138499353,\n",
       " 'jaccard@k': 0.24165068411226814,\n",
       " 'ndcg@k': 0.5956861368174092}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = np.load(\"artifacts/resultados/results_img.npy\", allow_pickle = True)\n",
    "\n",
    "# M√©dia final das m√©tricas\n",
    "mean_metrics_all = {\n",
    "    m: np.mean([res[m] for res in results_all])\n",
    "    for m in results[0]\n",
    "}\n",
    "\n",
    "mean_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8d36ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               Atelectasis       0.25      0.24      0.24    229036\n",
      "              Cardiomegaly       0.25      0.23      0.24    224222\n",
      "             Consolidation       0.06      0.06      0.06     53886\n",
      "                     Edema       0.18      0.17      0.17    135087\n",
      "Enlarged Cardiomediastinum       0.04      0.04      0.04     35894\n",
      "                  Fracture       0.02      0.02      0.02     21950\n",
      "               Lung Lesion       0.03      0.03      0.03     31419\n",
      "              Lung Opacity       0.25      0.24      0.25    257614\n",
      "                No Finding       0.43      0.46      0.44    377262\n",
      "          Pleural Effusion       0.32      0.29      0.30    271492\n",
      "             Pleural Other       0.01      0.01      0.01     10054\n",
      "                 Pneumonia       0.08      0.07      0.08     82773\n",
      "              Pneumothorax       0.07      0.07      0.07     51789\n",
      "           Support Devices       0.48      0.44      0.46    332780\n",
      "\n",
      "                 micro avg       0.30      0.29      0.29   2115258\n",
      "                 macro avg       0.18      0.17      0.17   2115258\n",
      "              weighted avg       0.30      0.29      0.29   2115258\n",
      "               samples avg       0.30      0.30      0.28   2115258\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ia368/miniconda3/envs/rag_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true_all = np.load(\"artifacts/resultados/y_true_img.npy\", allow_pickle = True)\n",
    "y_pred_all = np.load(\"artifacts/resultados/y_pred_img.npy\", allow_pickle = True)\n",
    "print(classification_report(y_true_all, y_pred_all, target_names = binary_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a2221dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estudos em que pelo menos um retorno tinha exatamente os mesmo labels: 69774/227835\n",
      "Estudos em que pelo menos um retorno tinha pelo menos um label igual: 181421/227835\n"
     ]
    }
   ],
   "source": [
    "at_least_one = 0\n",
    "one_with_label = 0\n",
    "for r in results:\n",
    "    if r['jaccard_1@k'] > 0:\n",
    "        at_least_one += 1\n",
    "    if r['jaccard@k'] > 0:\n",
    "        one_with_label += 1\n",
    "\n",
    "print(f'Estudos em que pelo menos um retorno tinha exatamente os mesmo labels: {at_least_one}/{len(results)}')\n",
    "print(f'Estudos em que pelo menos um retorno tinha pelo menos um label igual: {one_with_label}/{len(results)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
