{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e017369",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ffd6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import faiss\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b4fc3",
   "metadata": {},
   "source": [
    "# Importa vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caffb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_study = faiss.read_index(\"artifacts/vector_store/faiss_study.index\")  # Carregar o √≠ndice\n",
    "vs_img = faiss.read_index(\"artifacts/vector_store/faiss_img.index\")  # Carregar o √≠ndice\n",
    "vs_text = faiss.read_index(\"artifacts/vector_store/faiss_txt.index\")  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef19d5",
   "metadata": {},
   "source": [
    "# Fun√ß√µes de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2da675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_cases(query_emb, vector_store, ids, k=3):\n",
    "    \"\"\"\"\n",
    "    Pesquisa os estudos mais relevantes para uma determinada consulta.\n",
    "    Args:\n",
    "        - query_emb: embedding do estudo/imagem/texto de consulta\n",
    "        - vector_store: vector store\n",
    "        - k: n√∫mero de casos mais relevantes a serem retornados\n",
    "    Returns:\n",
    "        - IDs dos casos mais relevantes\n",
    "        - Casos mais relevantes\n",
    "    \"\"\"\n",
    "    # Preparar o vetor de consulta\n",
    "    if isinstance(query_emb, np.ndarray):\n",
    "        # Garantir que √© 2D e float32\n",
    "        if query_emb.ndim == 1:\n",
    "            query_embedding = query_emb.reshape(1, -1).astype(np.float32)\n",
    "        else:\n",
    "            query_embedding = query_emb.astype(np.float32)\n",
    "    else:\n",
    "        query_embedding = np.array(query_emb, dtype=np.float32).reshape(1, -1)\n",
    "    \n",
    "    # Normalizar o vetor de consulta\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    # Pesquisar os casos mais relevantes\n",
    "    D, I = vector_store.search(query_embedding, k)\n",
    "\n",
    "    # Retornar os IDs dos casos mais relevantes\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        results.append(ids[idx])\n",
    "        \n",
    "    return results, I[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08414d",
   "metadata": {},
   "source": [
    "## Teste para estudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55f25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.f_utils.embedding_utils import load_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd00df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835, 1152)\n"
     ]
    }
   ],
   "source": [
    "studies_emb = load_embeddings(\"artifacts/embeddings/e_study.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ce05ecce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(studies_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b8bcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835,)\n"
     ]
    }
   ],
   "source": [
    "study_ids = load_embeddings(\"artifacts/embeddings/study_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff673f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_estudo = studies_emb[0]  # Exemplo de estudo para teste\n",
    "ex_estudo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0d81728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Estudos mais relevantes: ['s50414267', 's53634677', 's52231919']\n",
      "- √çndices dos estudos mais relevantes: [    0 81535 56599]\n"
     ]
    }
   ],
   "source": [
    "estudos, idx = search_relevant_cases(ex_estudo, vs_study, study_ids, k=3)\n",
    "print(f\"- Estudos mais relevantes: {estudos}\")\n",
    "print(f\"- √çndices dos estudos mais relevantes: {idx}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459fc8ae",
   "metadata": {},
   "source": [
    "## Teste para imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec67e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835, 1152)\n"
     ]
    }
   ],
   "source": [
    "images_emb = load_embeddings(\"artifacts/embeddings/e_img.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da05b906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_img = images_emb[0]  # Exemplo de imagem para teste\n",
    "ex_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70cc8ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Imagens mais relevantes pertencem aos estudos: ['s59408044', 's54168089', 's57378102']\n",
      "- √çndices das imagens mais relevantes: [163390 101213  33199]\n"
     ]
    }
   ],
   "source": [
    "imagens, idx_img = search_relevant_cases(ex_img, vs_img, study_ids, k=3)\n",
    "print(f\"- Imagens mais relevantes pertencem aos estudos: {imagens}\")\n",
    "print(f\"- √çndices das imagens mais relevantes: {idx_img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677fda2",
   "metadata": {},
   "source": [
    "## Teste para texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f073f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings carregados com sucesso!\n",
      "üìä Formato dos dados: <class 'numpy.ndarray'>\n",
      "üìä Shape: (227835, 1152)\n"
     ]
    }
   ],
   "source": [
    "texts_emb = load_embeddings(\"artifacts/embeddings/e_text.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25916cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_txt = texts_emb[0]  # Exemplo de texto para teste\n",
    "ex_txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbd74b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Laudos mais relevantes pertencem aos estudos: ['s50414267', 's58823460', 's53634677']\n",
      "- √çndices dos laudos mais relevantes: [     0 221444  81535]\n"
     ]
    }
   ],
   "source": [
    "laudos, idx_txt = search_relevant_cases(ex_txt, vs_text, study_ids, k=3)\n",
    "print(f\"- Laudos mais relevantes pertencem aos estudos: {laudos}\")\n",
    "print(f\"- √çndices dos laudos mais relevantes: {idx_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab384a5",
   "metadata": {},
   "source": [
    "# Fun√ß√£o: retorno de estudo a partir de estudo de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c211b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_findings(self, report_text):\n",
    "        \"\"\"\n",
    "        Extrai o texto entre 'FINDINGS:' e a pr√≥xima se√ß√£o.\n",
    "        Caso n√£o encontre, retorna o texto completo truncado.\n",
    "        \"\"\"\n",
    "        findings = \"\"\n",
    "        try:\n",
    "            match = re.search(r\"FINDINGS:(.*?)(?:IMPRESSION:|CONCLUSION:|$)\", report_text, flags=re.S | re.I)\n",
    "            if match:\n",
    "                findings = match.group(1).strip()\n",
    "            else:\n",
    "                # Normaliza quebras de linha para \\n\n",
    "                text = report_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "\n",
    "                # Divide em blocos por linhas em branco (um ou mais \\n com espa√ßos possivelmente)\n",
    "                blocks = re.split(r\"\\n\\s*\\n+\", text)\n",
    "\n",
    "                best_block = \"\"\n",
    "                best_score = -1\n",
    "\n",
    "                for block in blocks:\n",
    "                    b = block.strip()\n",
    "\n",
    "                    # Remove um poss√≠vel cabe√ßalho \"T√çTULO:\" no in√≠cio do bloco\n",
    "                    # (t√≠tulos normalmente em mai√∫sculas, n√∫meros e s√≠mbolos comuns)\n",
    "                    b_clean = re.sub(r\"^\\s*[A-Z0-9 ,./()\\-]+:\\s*\", \"\", b)\n",
    "\n",
    "                    # Se ficar vazio, volta ao bloco original\n",
    "                    if not b_clean:\n",
    "                        b_clean = b\n",
    "\n",
    "                    # Calcula um \"score\" para decidir o maior bloco:\n",
    "                    # - comprimento do texto sem colapsar as quebras (para preservar formato)\n",
    "                    score = len(b_clean)\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_block = b_clean\n",
    "\n",
    "                findings = best_block.strip()\n",
    "        except:\n",
    "            findings = report_text.strip()\n",
    "        return findings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_text(text,model,tokenizer):\n",
    "    \"\"\"\n",
    "    Extrai embeddings de um texto usando um modelo de linguagem pr√©-treinado.\n",
    "    Args:\n",
    "        - text: texto a ser tranformado em embedding (assume que o pr√©-processamento j√° foi feito).\n",
    "        - tokenizer: tokenizador do modelo.\n",
    "        - model: modelo de linguagem pr√©-treinado.\n",
    "    Returns:\n",
    "        - embedding: embedding do texto.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenizar o texto\n",
    "    tokens = tokenizer(text, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # Passar os tokens pelo modelo\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_text_features(**tokens)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55b3524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "with open(\"configs/configs.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = AutoModel.from_pretrained(config['processor']['model'], token=config['processor']['auth_token']).to('cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['processor']['model'], token=config['processor']['auth_token'])\n",
    "processor = AutoProcessor.from_pretrained(config['processor']['model'], token=config['processor']['auth_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c0ae5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0225,  0.0067, -0.0090,  ..., -0.0192, -0.0099, -0.0047]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_embeddings_from_text(\"Teste\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28f69cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_img(imgs,model,processor):\n",
    "    \"\"\"\n",
    "    Extrai embeddings de uma imagem usando um modelo de linguagem pr√©-treinado.\n",
    "    Args:\n",
    "        - imgs: lista de imagens .jpg a ser tranformada em embedding\n",
    "        - model: modelo de linguagem pr√©-treinado\n",
    "        - processor: processador do modelo\n",
    "    Returns:\n",
    "        - embeddings: lista embeddings das imagens\n",
    "    \"\"\"\n",
    "\n",
    "    emb_imgs = []\n",
    "    for img in imgs:\n",
    "        # Carregar a imagem\n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "        # processar a imagem\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "        # Passar os tokens pelo modelo\n",
    "        with torch.no_grad():\n",
    "            emb = model.get_image_features(**inputs)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        emb_imgs.append(emb)\n",
    "\n",
    "    return emb_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "825af3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0110,  0.0186,  0.0156,  ..., -0.0484, -0.0240, -0.0201]])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagem = [\"../dados/mimic/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg\"]\n",
    "extract_embeddings_from_img(imagem,model,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be0ae5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding_single_study(texto, imagens, model, tokenizer, processor, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Extrai embeddings de texto e imagens a partir de um de estudo.\n",
    "    Args:\n",
    "        - texto: texto do estudo (laudo - partindo do pre suposto que j√° passou por extract_findings)\n",
    "        - imagens: lista de imagens do estudo\n",
    "        - alpha: peso de ponderacao dos embeddings de texto e imagem  \n",
    "    \"\"\"\n",
    "\n",
    "    # === 1) tokens e embeddings do texto ===\n",
    "    emb_text = extract_embeddings_from_text(texto, tokenizer=tokenizer, model=model)#.detach().numpy()  # embedding do texto\n",
    "\n",
    "    # === 2) tokens e embeddings das imagens ===\n",
    "    emb_images = extract_embeddings_from_img(imagens, model=model, processor=processor)\n",
    "\n",
    "    # === 3) faz pooling com imagens de entrada\n",
    "    # Stacking para [N, D]\n",
    "    emb_images = torch.stack(emb_images)  # [num_imagens, embedding_dim]\n",
    "    \n",
    "    # Pooling (m√©dia) ao longo das imagens\n",
    "    emb_pool = emb_images.mean(dim=0)  # [embedding_dim]\n",
    "    emb_pool = emb_pool / emb_pool.norm(dim=-1, keepdim=True)\n",
    "    #print(emb_pool.shape)\n",
    "\n",
    "    # === 4) fez media dos embeddings para embedding final\n",
    "    e_study = alpha * emb_text + (1 - alpha) * emb_pool\n",
    "    e_study = e_study / e_study.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return e_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_study_from_path(study_id, root_dir = \"..dados/mimic/files\"):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o que carrega laudo e imagens a partir de um caminho de pasta de estudo.\n",
    "    Args:\n",
    "        - study_id: pasta do estudo\n",
    "    Returns:\n",
    "        - dicionario de laudo e imagens\n",
    "    \"\"\"\n",
    "\n",
    "    # Procurar o arquivo em todos os subdiret√≥rios\n",
    "    text_path = ''\n",
    "    pastas_p = [nome for nome in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, nome))]\n",
    "    for dirpath in pastas_p:\n",
    "        pastas_pacientes = [nome for nome in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, nome))]\n",
    "\n",
    "        if f\"{study_id}.txt\" in filenames:\n",
    "            text_path = os.path.join(dirpath, f'{study_id}.txt')\n",
    "            print(f\"Arquivo encontrado: {text_path}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Arquivo n√£o encontrado.\")\n",
    "\n",
    "    # Procurar pasta do e listar arquivos (imagens)\n",
    "    target_folder = study_id\n",
    "    imagens = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if target_folder in dirnames:\n",
    "            folder_path = os.path.join(dirpath, target_folder)\n",
    "            arquivos = os.listdir(folder_path)\n",
    "            print(f\"Pasta encontrada: {folder_path}\")\n",
    "            print(\"Arquivos:\")\n",
    "            for arquivo in arquivos:\n",
    "                img = os.path.join(folder_path, arquivo)\n",
    "                print(img)\n",
    "                imagens.append(img)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Pasta n√£o encontrada.\")\n",
    "\n",
    "    estudo = {\n",
    "        'imgens': imagens,\n",
    "        'laudo': text_path\n",
    "    }\n",
    "\n",
    "    return estudo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3397c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laudo': 'There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\\n nodular opacities that most likely represent nipple shadows. The\\n cardiomediastinal silhouette is normal.  Clips project over the left lung,\\n potentially within the breast. The imaged upper abdomen is unremarkable.\\n Chronic deformity of the posterior left sixth and seventh ribs are noted.',\n",
       " 'imagens': ['../dados/mimic/files/p10/p10000032/s50414267/174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg',\n",
       "  '../dados/mimic/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg']}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caminho = '../dados/mimic/files/p10/p10000032/'\n",
    "estudo = 's50414267'\n",
    "\n",
    "# Carregar texto do laudo\n",
    "with open(caminho + estudo + '.txt', 'r') as f:\n",
    "    texto = f.read()\n",
    "\n",
    "# Passar pelo m√©todo de extra√ß√£o de findings\n",
    "findings = _extract_findings(None, texto)  # Se for m√©todo de classe, ajuste para self ou use como fun√ß√£o\n",
    "\n",
    "exemplo = {\n",
    "    'laudo': findings,\n",
    "    'imagens': [caminho+estudo+'/'+f for f in os.listdir(caminho + estudo) if f.lower().endswith(\".jpg\")]\n",
    "}\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2cc56291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0489,  0.0295, -0.0231,  ..., -0.0157, -0.0255, -0.0183]]),\n",
       " tensor([[-0.0110,  0.0186,  0.0156,  ..., -0.0484, -0.0240, -0.0201]])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_embeddings_from_img(exemplo['imagens'], model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5b671995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0141,  0.0146, -0.0266,  ..., -0.0346, -0.0764, -0.0155]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_embeddings_from_text(exemplo['laudo'],model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a2e7b9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1152)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_embedding_single_study(\n",
    "    texto=exemplo['laudo'],\n",
    "    imagens=exemplo['imagens'],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    alpha=0.5\n",
    ").detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cef6c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Estudos mais relevantes: ['s50414267', 's58823460', 's58997875']\n",
      "- √çndices dos estudos mais relevantes: [     0 221444   5166]\n"
     ]
    }
   ],
   "source": [
    "emb_estudo_exemplo = extract_embedding_single_study(\n",
    "    texto=exemplo['laudo'],\n",
    "    imagens=exemplo['imagens'],\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    alpha=0.5\n",
    ").detach().cpu().numpy()\n",
    "\n",
    "estudos, idx = search_relevant_cases(emb_estudo_exemplo, vs_study, study_ids, k=3)\n",
    "print(f\"- Estudos mais relevantes: {estudos}\")\n",
    "print(f\"- √çndices dos estudos mais relevantes: {idx}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
